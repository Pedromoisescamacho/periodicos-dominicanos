fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% as.character()
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
data.frame(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
x <- leer_contenido("https://www.diariolibre.com/rss/portada.xml")
str(x)
sapply(diario_libre,leer_contenido)
x M- sapply(diario_libre,leer_contenido)
x <- sapply(diario_libre,leer_contenido)
str(x)
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% as.character()
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
c(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
x <- sapply(diario_libre,leer_contenido)
str(x)
str(diario_libre)
x[1]
x <- lapply(diario_libre,leer_contenido)
str(x)
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% as.character()
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
as.matrix(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
x <- lapply(diario_libre,leer_contenido)
dim(x)
summary(x)
str(x)
leer_contenido("https://www.diariolibre.com/rss/portada.xml")
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% as.character()
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
matrix(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
leer_contenido("https://www.diariolibre.com/rss/portada.xml")
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% as.character()
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
data.frame(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
x <- leer_contenido("https://www.diariolibre.com/rss/portada.xml")
y <- leer_contenido("https://www.diariolibre.com/rss/estilos.xml")
rbind(x,y)
str(x)
Sys.timezone()
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% with_tz("America/Caracas")
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
data.frame(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
x <- leer_contenido("https://www.diariolibre.com/rss/portada.xml")
y <- leer_contenido("https://www.diariolibre.com/rss/estilos.xml")
xy <- rbind(x,y)
str(xy)
df_raw <- lapply(diario_libre,leer_contenido)
str(df_raw)
do.call("complex", list(imag = 1:3))
df_raw <- lapply(diario_libre,leer_contenido) %>% do.call(rbind)
df_raw <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
str(df_raw)
diario_libre <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
name(diario_libre)
names(diario_libre)
str(diario_libre)
summary(diario_libre)
library(quanteda)
corpus <- corpus(diario_libre$cuerpo)
str(corpus)
cleanFun <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
diario_libre$cuerpo2 <- sapply(diario_libre$cuerpo, cleanFun)
diario_libre$cuerpo2[1]
diario_libre$cuerpo[1]
diario_libre <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
diario_libre <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
library(xml2)
library(dplyr)
library(lubridate)
library(quanteda)
cleanFun <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
diario_libre <- c("https://www.diariolibre.com/rss/portada.xml",
"https://www.diariolibre.com/rss/noticias.xml",
"https://www.diariolibre.com/rss/mundo.xml",
"https://www.diariolibre.com/rss/economia.xml",
"https://www.diariolibre.com/rss/opinion.xml",
"https://www.diariolibre.com/rss/ciencia-y-tecnologia.xml",
"https://www.diariolibre.com/rss/revista.xml",
"https://www.diariolibre.com/rss/deportes.xml",
"https://www.diariolibre.com/rss/estilos.xml")
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% with_tz("America/Caracas")
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
data.frame(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
diario_libre <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
diario_libre$cuerpo <- sapply(diario_libre$cuerpo, cleanFun)
diario_libre$cuerpo[1]
diario_libre$cuerpo[50]
corpus <- corpus(diario_libre$cuerpo)
stopwords("spanish")
str(stopwords())
n1 <- dfm(token1, stopwords("spnaish"))
token1 <- tokens(char_tolower(corpus), what = "word", remove_punct = TRUE, ngrams= 1, remove_numbers = TRUE, remove_symbols = TRUE, remove_twitter = TRUE, remove_url = TRUE)
n1 <- dfm(token1, stopwords("spnaish"))
n1 <- dfm(token1, stopwords("spanish"))
n1 <- dfm(token1, stopwords("spanish"))
n1 <- dfm(token1)
n1 <- dfm(token1, stopwords("spanish"))
stopwords("spanish")
n1 <- dfm(token1)
topfeatures(n1)
n1 <- dfm(token1, stopwords("spanish"))
topfeatures(n1)
n1 <- dfm(token1, remove = stopwords("spanish"))
topfeatures(n1)
topfeatures(n1, 50)
n1top <- dfm_trim(n1, 3) %>% nfeature() %>% topfeatures()
nfeature(n1)
n1top <- dfm_trim(n1, 3) %>% topfeatures(nfeature(n1))
n1top
n1top <- dfm_trim(n1, 10) %>% topfeatures(nfeature(n1))
n1top
token2 <- tokens(char_tolower(corpus), what = "word", remove_punct = TRUE,
ngrams= 2,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_twitter = TRUE,
remove_url = TRUE)
n2 <- dfm(token2)
n2top <- dfm_trim(n2, 10) %>% topfeatures(nfeature(n2))
n2top
n2 <- dfm(token2, remove = stopwords("spanish"))
token2 <- tokens(char_tolower(corpus), what = "word", remove_punct = TRUE,
ngrams= 2,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_twitter = TRUE,
remove_url = TRUE)
n2 <- dfm(token2, remove = stopwords("spanish"))
n2top <- dfm_trim(n2, 10) %>% topfeatures(nfeature(n2))
head(n2top)
token3 <- tokens(char_tolower(corpus), what = "word", remove_punct = TRUE,
ngrams= 3,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_twitter = TRUE,
remove_url = TRUE)
n3 <- dfm(token3, remove = stopwords("spanish"))
n3top <- dfm_trim(n3, 10) %>% topfeatures(nfeature(n3))
head(n3top)
n3top
n3top <- dfm_trim(n3, 2) %>% topfeatures(nfeature(n3))
head(n3top)
n3top <- dfm_trim(n3, 3) %>% topfeatures(nfeature(n3))
head(n3top)
n3top <- dfm_trim(n3, 2) %>% topfeatures(nfeature(n3))
head(n3top)
n3top
n3top <- dfm_trim(n3, 3) %>% topfeatures(nfeature(n3))
n3top <- dfm_trim(n3, 3) %>% topfeatures(nfeature(n3))
n3top
summary(corpus)
docvars(corpus, "fecha") <- diario_libre$fecha
docvars(corpus, "id noticia") <- diario_libre$id_noticia
summary(corpus)
tokenInfo <- summary(corpus)
require(ggplot2)
ggplot(data=tokenInfo, aes(x = fecha, y = Tokens, group = 1)) + geom_line() + geom_point()
table(diario_libre$fecha
)
table(as.Date(diario_libre$fecha))
diario_libre$titulo[as.Date(diario_libre$fecha) == "2015-07-27"]
diario_libre$titulo[as.Date(diario_libre$fecha) == "2015-10-0"]
diario_libre$titulo[as.Date(diario_libre$fecha) == "2015-10-07"]
ggplot(data=tokenInfo, aes(x = as.Date(fecha), y = Tokens, group = 1)) + geom_line() + geom_point(
)
range(diario_libre$fecha)
class(diario_libre$fecha)
max(diario_libre$fecha)
max(diario_libre$fecha) %>% as.Date() %>% -30
max(diario_libre$fecha) %>% as.Date() %>% -31
fecha_mi <- max(diario_libre$fecha) %>% as.Date() %>% -31
fecha_mi
diario_libre[diario_libre$fecha>=fecha_mi,]
dl_31 <- diario_libre[diario_libre$fecha>=fecha_mi,]
str(dl_31)
str(diario_libre)
dim(diario_libre)
dim(dl_31)
install.packages(c("shiny", "dplyr", "htmlwidgets", "digest", "bit"))
devtools::install_github("jcheng5/bubbles")
install.packages(c("shiny", "dplyr", "htmlwidgets", "digest", "bit"))
install.packages(c("shiny", "dplyr", "htmlwidgets", "digest", "bit"))
install.packages(c("shiny", "dplyr", "htmlwidgets", "digest", "bit"))
library(xml2)
library(dplyr)
library(lubridate)
library(quanteda)
##funcion para quitar los html tags
cleanFun <- function(htmlString) {
return(gsub("<.*?>", "", htmlString))
}
#### creando vector de las diferentes fuentes del diario libre
diario_libre <- c("https://www.diariolibre.com/rss/portada.xml",
"https://www.diariolibre.com/rss/noticias.xml",
"https://www.diariolibre.com/rss/mundo.xml",
"https://www.diariolibre.com/rss/economia.xml",
"https://www.diariolibre.com/rss/opinion.xml",
"https://www.diariolibre.com/rss/ciencia-y-tecnologia.xml",
"https://www.diariolibre.com/rss/revista.xml",
"https://www.diariolibre.com/rss/deportes.xml",
"https://www.diariolibre.com/rss/estilos.xml")
#### lista de palabras adicionales al stopword de español
#funcion para leer el contenido de cada fuente para el diario libre
leer_contenido <- function(fuente){
xml <- read_xml(fuente)
titulo <- xml %>% xml_find_all("//item//title") %>% xml_text()
cuerpo <- xml %>% xml_find_all("//item//description") %>% xml_text(trim = TRUE)
fecha <- xml %>% xml_find_all("//item//pubDate") %>% xml_text()  %>% strptime( "%a, %d %b %Y %T", tz = "GMT") %>% with_tz("America/Caracas")
id_noticia <- xml %>% xml_find_all("//item//id") %>% xml_text()
autor <- xml %>% xml_find_all("//item//author") %>% xml_text()
data.frame(titulo, cuerpo, fecha, id_noticia, autor, stringsAsFactors = FALSE)
}
#obteniendo lista de dataframes de cada fuente
diario_libre <- lapply(diario_libre,leer_contenido) %>% do.call(what = rbind)
diario_libre$cuerpo <- sapply(diario_libre$cuerpo, cleanFun)
diario_libre
head(diario_libre)
names(diario_libre)
install.packages("devtools")
library(devtools)
2
3
install_github("Rfacebook", "pablobarbera", subdir="Rfacebook")
require (Rfacebook)
fb_oauth <- fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
require("Rfacebook")
fb_oauth <- fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
install.packages("Rfacebook")
fb_oauth <- fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
fb_oauth <- fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
library("Rfacebook")
fb_oauth <- fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
fb_oauth <- Rfacebook::fbOAuth(app_id="900224733468542", app_secret="bedffd32d84630eab7dccfdc6b3af772",extended_permissions = TRUE)
library(Rfacebook)
library(httr)
install.packages("httr")
library(httr)
ld_fb_url <- "https://raw.githubusercontent.com/Pedromoisescamacho/periodicos-dominicanos/master/datasets_noticias/fb_listin.csv"
ld_df <- read.csv(dl_fb_url, stringsAsFactors = F)
ld_fb_url <- "https://raw.githubusercontent.com/Pedromoisescamacho/periodicos-dominicanos/master/datasets_noticias/fb_listin.csv"
ld_df <- read.csv(dl_fb_url, stringsAsFactors = F)
ld_df <- read.csv(ld_fb_url, stringsAsFactors = F)
dl_url <- dl_df$url
ld_url <- ld_df$url
ld_url
noticia <- read_html("https://goo.gl/sKsn5y")
library(xml2)
library(dplyr)
library(rvest)
noticia <- read_html("https://goo.gl/sKsn5y")
contenido <- noticia %>% html_nodes("#ArticleBody p") %>% html_text
contenido
contenido <- noticia %>% html_nodes("#ArticleBody p") %>% html_text %>% do.call(what = paste)
contenido
contenido %>% class()
str(contenido)
contenido <- noticia %>% html_nodes("#ArticleBody p") %>% html_text %>% as.list %>% do.call(what = paste)
contenido
contenidos <- sapply(ld_url[1:6], contenido)
contenido <- function(url){
content <- function(url){
noticia <- read_html(url)
contenido <- noticia %>% html_nodes("#ArticleBody") %>% html_text %>% as.list() %>% do.call(what = paste)
contenido <- ifelse(identical(contenido, character(0)), "NULL", contenido)
contenido
}
return(tryCatch(content(url), error = function(e) NULL))
}
contenidos <- sapply(ld_url[1:6], contenido)
contenidos
contenidos[1]
contenidos[2]
contenidos[3]
print(contenidos[3])
str(contenidos)
str(contenidos[3])
contenido(ld_url[3])
contenido(ld_url[3]) %>% print
noticia <- read_html(ld_url[3])
contenido <- noticia %>% html_nodes('div id="ArticleSourceDiv"') %>% html_text %>% as.list %>% do.call(what = paste)
contenido <- noticia %>% html_nodes('div id="ArticleBody"') %>% html_text %>% as.list %>% do.call(what = paste)
View(contenidos[3])
library(microbenchmark)
microbenchmark(contenido(ld_url[3]))
2600/60
contenidos <- sapply(ld_url[1:6], contenido)
contenidos <- sapply(ld_url, contenido)
contenidos <- sapply(ld_url, contenido)
for (url in ld_url){
cont <- contenido(url)
content <- c(content, cont)
}
content <- c()
for (url in ld_url){
cont <- contenido(url)
content <- c(content, cont)
}
content %>% class()
content %>% class()
content[1:3]
length(content)
length(content[content == "NULL"])
length(content[content == NULL])
286/3476
length(ld_url)
noticia <- read_html(ld_url[3])
contenido <- noticia %>% html_nodes('div id="ArticleBody"') %>% html_text %>% as.list %>% do.call(what = paste)
contenido()
contenido
conte <- noticia %>% html_nodes('div id="ArticleBody"') %>% html_text %>% as.list %>% do.call(what = paste)
conte
ld_url[3]
contenido <- function(url){
content <- function(url){
noticia <- read_html(url)
contenido <- noticia %>% html_nodes("#ArticleBody") %>% html_text %>% as.list() %>% do.call(what = paste)
contenido <- ifelse(identical(contenido, character(0)), "NULL", contenido)
contenido
}
return(tryCatch(content(url), error = function(e) NULL))
}
contenido(ld_url[3])
div id="ArticleBody
conte <- noticia %>% html_nodes('.art_titulo"') %>% html_text
noticia <- read_html(ld_url[3])
conte <- noticia %>% html_nodes('.art_titulo"') %>% html_text
conte
conte <- noticia %>% html_nodes('.art_titulo"') %>% html_text
conte <- noticia %>% html_nodes(".art_titulo") %>% html_text
conte
conte <- noticia %>% html_nodes(".art_titulo") %>% html_text
conte <- noticia %>% html_nodes(".art_titulo") %>% html_text
conte
titulo <- function(url){
title <- function(url){
noticia <- read_html(url)
title <- noticia %>% html_nodes(".art_titulo") %>% html_text
title <- ifelse(identical(title, character(0)), "NULL", title)
title
}
return(tryCatch(title(url), error = function(e) NULL))
}
titulo(ld_url[4])
titulos <- sapply(ld_url[1:5], titulo)
titulos
conte <- noticia %>% html_nodes("span class") %>% html_text
conte
conte <- noticia %>% html_nodes("<span class>") %>% html_text
conte
conte <- noticia %>% html_nodes("span class") %>% html_text
conte
conte <- noticia %>% html_nodes("#article_zone span") %>% html_text
conte
conte <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
conte
noticia <- read_html(ld_url[3])
conte <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
noticia <- read_html(ld_url[3])
conte <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
conte
noticia <- read_html(ld_url[7])
conte <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
conte
pattern <- "[0-9].*M"
pattern <- "[a-z].*[0-9]"
regexpr(pattern, conte)
regmatches(conte, regexpr(pattern, fecha_cruda))
regmatches(conte, regexpr(pattern, conte))
fecha <- function(url){
title <- function(url){
noticia <- read_html(url)
pattern <- "[a-z].*[0-9]"
fecha_cruda <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
fecha <- regmatches(fecha_cruda, regexpr(pattern, fecha_cruda)) #%>%  as.POSIXct(format = "%d %b %Y, %H:%M %p")
fecha
}
return(tryCatch(title(url), error = function(e) NULL))
}
fecha(ld_url[2])
fecha(ld_url[4])
fecha <- function(url){
date <- function(url){
noticia <- read_html(url)
pattern <- "[a-z].*[0-9]"
fecha_cruda <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
fecha <- regmatches(fecha_cruda, regexpr(pattern, fecha_cruda)) #%>%  as.POSIXct(format = "%d %b %Y, %H:%M %p")
fecha
}
return(tryCatch(date(url), error = function(e) NULL))
}
fecha(ld_url[4])
fecha
fechas
titulos
names(titulos)
contenidos <- sapply(dl_url[1:4], contenido)
contenidos <- sapply(ld_url[1:4], contenido)
names(contenidos)
contenidos %>% str()
fecha <- function(url){
date <- function(url){
noticia <- read_html(url)
pattern <- "[a-z].*[0-9]"
fecha_cruda <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
fecha <- regmatches(fecha_cruda, regexpr(pattern, fecha_cruda)) #%>%  as.POSIXct(format = "%d %b %Y, %H:%M %p")
fecha <- ifelse(identical(fecha, character(0)), "NULL", fecha)
fecha
}
return(tryCatch(date(url), error = function(e) NULL))
}
fechas <- sapply(ld_url[1:5], fecha)
fechas
titulos <- sapply(ld_url[1:5], titulo)
titulos
titulos %>% str()
fechas %>% str()
contenidos <- sapply(ld_url, contenido)
titulos <- sapply(ld_url, titulo)
fechas <- sapply(ld_url, fecha)
ld_url  %>% length()
contenidos %>% length()
fechas %>% length()
titulos %>% length()
names(titulos) %>% head()
names(fechas) %>% head()
names(fechas) == names(titulos)
any(!(names(fechas) == names(titulos)))
any((names(fechas) == names(titulos)))
identical(names(fechas) == names(titulos))
identical(names(fechas), names(titulos))
identical(names(fechas), names(contenidos))
contenido <- function(url){
content <- function(url){
noticia <- read_html(url)
contenido <- noticia %>% html_nodes("#ArticleBody") %>% html_text %>% as.list() %>% do.call(what = paste)
contenido <- ifelse(identical(contenido, character(0)), "NULL", contenido)
contenido
}
return(tryCatch(content(url), error = function(e) "NULL"))
}
titulo <- function(url){
title <- function(url){
noticia <- read_html(url)
title <- noticia %>% html_nodes(".art_titulo") %>% html_text
title <- ifelse(identical(title, character(0)), "NULL", title)
title
}
return(tryCatch(title(url), error = function(e) "NULL"))
}
fecha <- function(url){
date <- function(url){
noticia <- read_html(url)
pattern <- "[a-z].*[0-9]"
fecha_cruda <- noticia %>% html_nodes(".art_sly_1 span") %>% html_text
fecha <- regmatches(fecha_cruda, regexpr(pattern, fecha_cruda)) #%>%  as.POSIXct(format = "%d %b %Y, %H:%M %p")
fecha <- ifelse(identical(fecha, character(0)), "NULL", fecha)
fecha
}
return(tryCatch(date(url), error = function(e) "NULL"))
}
contenidos <- sapply(ld_url, contenido)
titulos <- sapply(ld_url, titulo)
fechas <- sapply(ld_url, fecha)
fechas[sapply(fechas, is.null)]
fechas[sapply(fechas, is.null)] <- "NULL"
fechas %>% unlist() %>% c() %>% length()
fechas2 <- fechas %>% unlist() %>% c()
noticias <- data.frame(contenidos, fechas2, titulos, stringsAsFactors = F)
noticias %>% dim()
setwd("~/R/project/periodicos dominicanos/periodicos-dominicanos")
write.csv(noticias, "./noticias csv/listin_diario_noticias.csv")
